{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Preparing the dataset\n",
    "After analyzing our dataset, we need to prepare it for model training. The prepare_dataset function stored in functions.py handles this by cleaning and organizing our data. It keeps only the essential columns (ImgId and categories), creates backups of the original data, and performs consistency checks between the CSV entries and actual image files. This function also provides detailed information about the dataset's shape, category distribution, and any potential inconsistencies, helping us ensure our data is properly structured before proceeding with feature extraction and model training."
   ],
   "id": "bd6fd513ba73e828"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def prepare_dataset(data_dir, CSV_PATH, images_dir, backup_dir):\n",
    "    \"\"\"\n",
    "    Prepare dataset by cleaning and organizing data\n",
    "    \"\"\"\n",
    "    # Create backup directory\n",
    "    os.makedirs(backup_dir, exist_ok=True)\n",
    "\n",
    "    print(\"Loading dataset...\")\n",
    "    df = pd.read_csv(CSV_PATH)\n",
    "\n",
    "    # Print initial shape\n",
    "    print(f\"\\nInitial dataset shape: {df.shape}\")\n",
    "    print(\"Initial columns:\", df.columns.tolist())\n",
    "\n",
    "    # Keep only essential columns\n",
    "    keep_columns = ['ImgId', 'categories']\n",
    "    df_filtered = df[keep_columns]\n",
    "\n",
    "    print(\"\\nAfter filtering:\")\n",
    "    print(f\"Final dataset shape: {df_filtered.shape}\")\n",
    "    print(\"\\nCategory distribution:\")\n",
    "    print(df_filtered['categories'].value_counts())\n",
    "\n",
    "    # Save CSVs\n",
    "    backup_CSV_PATH = os.path.join(backup_dir, 'styles_original.csv')\n",
    "    df.to_csv(backup_CSV_PATH, index=False)  # Backup\n",
    "    df_filtered.to_csv(CSV_PATH, index=False)  # Filtered\n",
    "    print(f\"\\nSaved backup to: {backup_CSV_PATH}\")\n",
    "    print(f\"Saved filtered data to: {CSV_PATH}\")\n",
    "\n",
    "    # Return dictionary with all results\n",
    "    results = {\n",
    "        'original_shape': df.shape,\n",
    "        'filtered_shape': df_filtered.shape,\n",
    "        'kept_columns': df_filtered.columns.tolist(),\n",
    "        'category_distribution': df_filtered['categories'].value_counts(),\n",
    "        'missing_values': df_filtered.isnull().sum()\n",
    "    }\n",
    "\n",
    "    # Add consistency check results\n",
    "    print(\"\\nPerforming consistency checks...\")\n",
    "    remaining_images = set(os.path.splitext(f)[0] for f in os.listdir(images_dir) if f.endswith('.jpg'))\n",
    "    csv_ids = set(df_filtered['ImgId'].astype(str))\n",
    "\n",
    "    results['inconsistencies'] = {\n",
    "        'missing_images': csv_ids - remaining_images,\n",
    "        'extra_images': remaining_images - csv_ids\n",
    "    }\n",
    "\n",
    "    print(f\"Missing images: {len(results['inconsistencies']['missing_images'])}\")\n",
    "    print(f\"Extra images: {len(results['inconsistencies']['extra_images'])}\")\n",
    "\n",
    "    return results"
   ],
   "id": "7fbbbe0bbbacd509"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Connect dataset and csv file\n",
    "The connect_dataset function verifies the relationship between our CSV metadata and actual image files, ensuring data integrity before model training. It creates image paths from ImgId values, validates each image's existence and readability, and maintains only valid entries. The function identified 4,229 missing images, matching our earlier analysis, and produced a clean dataset of 42,000 valid images with their corresponding categories ready for feature extraction.\n"
   ],
   "id": "790f6559fa7cef16"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def connect_dataset(CSV_PATH, IMAGES_DIR):\n",
    "    \"\"\"\n",
    "    Connect the image dataset by linking the CSV metadata with actual image files.\n",
    "\n",
    "    Parameters:\n",
    "    CSV_PATH (str): Path to the CSV file containing image metadata\n",
    "    IMAGES_DIR (str): Path to the folder containing image files\n",
    "\n",
    "    Returns:\n",
    "    tuple: (DataFrame with verified image paths, list of any missing images)\n",
    "    \"\"\"\n",
    "    print(\"Reading CSV metadata...\")\n",
    "    metadata_df = pd.read_csv(CSV_PATH)\n",
    "\n",
    "    metadata_df['image_path'] = metadata_df['ImgId'].apply(\n",
    "        lambda x: os.path.join(IMAGES_DIR, f\"{x}.jpg\")\n",
    "    )\n",
    "\n",
    "    print(\"Verifying image files...\")\n",
    "    missing_images = []\n",
    "    existing_images = []\n",
    "\n",
    "    for idx, row in metadata_df.iterrows():\n",
    "        if idx % 1000 == 0:\n",
    "            print(f\"Checking image {idx} of {len(metadata_df)}\")\n",
    "\n",
    "        if os.path.exists(row['image_path']):\n",
    "            try:\n",
    "                with Image.open(row['image_path']) as img:\n",
    "                    existing_images.append(True)\n",
    "            except Exception as e:\n",
    "                print(f\"Error with image {row['ImgId']}: {str(e)}\")\n",
    "                existing_images.append(False)\n",
    "                missing_images.append(row['ImgId'])\n",
    "        else:\n",
    "            existing_images.append(False)\n",
    "            missing_images.append(row['ImgId'])\n",
    "\n",
    "    metadata_df['image_exists'] = existing_images\n",
    "    valid_df = metadata_df[metadata_df['image_exists']][['ImgId', 'categories', 'image_path']].copy()\n",
    "\n",
    "    print(\"\\nDataset Summary:\")\n",
    "    print(f\"Total entries in CSV: {len(metadata_df)}\")\n",
    "    print(f\"Valid images found: {len(valid_df)}\")\n",
    "    print(f\"Missing images: {len(missing_images)}\")\n",
    "\n",
    "    return valid_df, missing_images"
   ],
   "id": "12d10d728e0e3b37"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
